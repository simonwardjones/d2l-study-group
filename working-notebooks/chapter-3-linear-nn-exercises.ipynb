{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 - linear neural networks - Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## [3.1.6.] Exercises (Linear regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Assume that we have some data  $x_1,...x_n\\in\\mathbb{R}$. Our goal is to find a constant $b$  such that  $\\sum_i(x_i-b)^2$  is minimized.\n",
    "\n",
    "1) Find a analytic solution for the optimal value of $b$  \n",
    "2) How does this problem and its solution relate to the normal distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial b} \\left(\\sum_i(x_i-b)^2\\right)\n",
    "&= \\sum_i\\frac{\\partial}{\\partial b}(x_i-b)^2\\\\\n",
    "&= - \\sum_i 2 (x_i-b)\n",
    "\\end{aligned}\n",
    "$$\n",
    "Solving for $b$ where the derivative is 0\n",
    "$$\n",
    "\\frac{\\partial}{\\partial b} \\left(\\sum_i(x_i-b)^2\\right) = 0\n",
    "$$\n",
    "gives\n",
    "$$b = \\frac{1}{n}\\sum_ix_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If we assume that we are trying to measure a true value $b$. However if we assume that the error $\\epsilon$ in our measurements are normally distributed $\\epsilon=b-x_i\\sim\\mathcal{N}(0,\\sigma)$. Then\n",
    "$$P(\\epsilon=0|x_i)=\\prod_i\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^\\left(\\frac{-(b-x_i)^2}{2\\sigma^2}\\right)$$\n",
    "\n",
    "Minimising negative log likelihood is equivalent to minimising the square errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Derive the analytic solution to the optimization problem for linear regression with squared error. To keep things simple, you can omit the bias $b$ from the problem (we can do this in principled fashion by adding one column to $\\mathbf{X}$  consisting of all ones).\n",
    "\n",
    "1) Write out the optimization problem in matrix and vector notation (treat all the data as a single matrix, and all the target values as a single vector).  \n",
    "2) Compute the gradient of the loss with respect to $\\mathbf{w}$.  \n",
    "3) Find the analytic solution by setting the gradient equal to zero and solving the matrix equation.  \n",
    "4) When might this be better than using stochastic gradient descent? When might this method break?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We wish to find $\\mathbf{w}$ such that $\\mathbf{\\hat{y}}= \\mathbf{X}\\mathbf{w}$ where the first column of $\\mathbf{X}$ is all ones and the first weight $\\mathbf{w}_0$ is the bias.\n",
    "\n",
    "In order to find optimal weights $\\mathbf{w}^*$ we use gradient descent and update $\\mathbf{w}$ in the direction of the negative gradient of the loss function $\\mathcal{L}(\\mathbf{X},\\mathbf{w})$ with respect to $\\mathbf{w}$ where we treat $\\mathbf{X}$ as fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Hence the update is\n",
    "$$\n",
    "\\mathbf{w}^{n+1} \\leftarrow \\mathbf{w}^{n} - \\eta \\nabla_\\mathbf{w}\\mathcal{L}\n",
    "$$\n",
    "where \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_\\mathbf{w}\\mathcal{L} \n",
    "&= \\nabla_\\mathbf{w}(\\mathbf{\\hat{y}} - \\mathbf{y})^T(\\mathbf{\\hat{y}} - \\mathbf{y})\\\\\n",
    "&= \\nabla_\\mathbf{w}(\\mathbf{\\hat{y}}^T\\mathbf{\\hat{y}} - 2\\mathbf{y}^T\\mathbf{\\hat{y}} + \\mathbf{y}^T\\mathbf{y})\\\\\n",
    "&= \\nabla_\\mathbf{w}(\\mathbf{w}^T\\mathbf{X}^T\\mathbf{X}\\mathbf{w} - 2\\mathbf{y}^T\\mathbf{X}\\mathbf{w} + \\mathbf{y}^T\\mathbf{y})\\\\\n",
    "&= 2\\mathbf{X}^T\\mathbf{X}\\mathbf{w}- 2\\mathbf{X}^T\\mathbf{y}\\\\\n",
    "&= 2\\mathbf{X}^T(\\mathbf{\\hat{y}} - \\mathbf{y})\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Setting the gradient of the loss equal to 0 we see\n",
    "$$\n",
    "0 = 2\\mathbf{X}^T\\mathbf{X}\\mathbf{w}- 2\\mathbf{X}^T\\mathbf{y}\n",
    "\\\\\\implies\n",
    "\\mathbf{X}^T\\mathbf{X}\\mathbf{w} = \\mathbf{X}^T\\mathbf{y}\n",
    "\\\\\\implies\n",
    "\\mathbf{X}\\mathbf{w} = (\\mathbf{X}^T)^{-1}\\mathbf{X}^T\\mathbf{y}\n",
    "\\\\\\implies\n",
    "\\mathbf{w} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This method might be better for small data as it will be faster. This method might break down if there is no inverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Assume that the noise model governing the additive noise $\\epsilon$  is the exponential distribution. That is, \n",
    "$$p(\\epsilon) = \\frac{1}{2}e^{-|\\epsilon|}$$\n",
    "\n",
    "Write out the negative log-likelihood of the data under the model $-\\log P(\\mathbf{y}|\\mathbf{X})$\n",
    "\n",
    "Can you find a closed form solution?\n",
    "\n",
    "Suggest a stochastic gradient descent algorithm to solve this problem. What could possibly go wrong (hint: what happens near the stationary point as we keep on updating the parameters)? Can you fix this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "P(\\mathbf{y}|\\mathbf{X}) = \\prod_i P(y^i|\\mathbf{x}_i) = \\prod_i \\frac{1}{2}e^{-|y_i - \\mathbf{w}^T\\mathbf{x}_i-b|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "hence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "- \\log P(\\mathbf{y}|\\mathbf{X}) = - \\sum_i \\log(\\frac{1}{2}) + \\sum_i |y_i - \\mathbf{w}^T\\mathbf{x}_i-b|\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The stochastic gradient descent problem could be minimising\n",
    "\n",
    "$$\n",
    "\\sum_i |y_i - \\mathbf{w}^T\\mathbf{x}_i-b|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## xxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "d2l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
