\documentclass[12pt,notitlepage]{article}

% for margins
\usepackage[a4paper,left=3cm,right=2cm,top=2.5cm,bottom=2.5cm]{geometry}
% for citations
\usepackage{apacite}
% for hyperlinks
\usepackage{hyperref}
% for maths symbols like the real numbers
\usepackage{amssymb}


\begin{document}
\bibliographystyle{apacite}

\title{\Large{\textbf{Chapter 7}}}
\date{Novemeber 1, 2020}
\author{Simon Ward-Jones\\simonwardjones16@gmail.com}

\maketitle
% \tableofcontents
\href{https://d2l.ai/chapter_convolutional-modern/index.html}{Link to chapter}

\section{Modern Convolutional Neural Networks}

\section{AlexNet - 2012}
\begin{itemize}
  \item AlexNet has a similar structure to that of LeNet, but uses more convolutional layers and a larger parameter space to fit the large-scale ImageNet dataset
  \item Dropout, ReLU, and preprocessing were the other key steps in achieving excellent performance in computer vision tasks
\end{itemize}

\section{ Networks Using Blocks (VGG) - 2014 }
\begin{itemize}
  \item VGG-11 constructs a network using reusable convolutional blocks. Different VGG models can be defined by the differences in the number of convolutional layers and output channels in each block.
  \item The use of blocks leads to very compact representations of the network definition. It allows for efficient design of complex networks.
\end{itemize}

\section{ Network in Network (NiN) - 2014 }
\begin{itemize}
  \item NiN uses blocks consisting of a convolutional layer and multiple  1×1  convolutional layers. This can be used within the convolutional stack to allow for more per-pixel nonlinearity.
  \item NiN removes the fully-connected layers and replaces them with global average pooling (i.e., summing over all locations) after reducing the number of channels to the desired number of outputs (e.g., 10 for Fashion-MNIST).
\end{itemize}

\section{ Networks with Parallel Concatenations (GoogLeNet) - 2015 }
\begin{itemize}
  \item The Inception block is equivalent to a subnetwork with four paths. It extracts information in parallel through convolutional layers of different window shapes and maximum pooling layers.  1×1  convolutions reduce channel dimensionality on a per-pixel level. Maximum pooling reduces the resolution.
  \item GoogLeNet connects multiple well-designed Inception blocks with other layers in series. The ratio of the number of channels assigned in the Inception block is obtained through a large number of experiments on the ImageNet dataset.
\end{itemize}

\section{Batch Normalization}
\begin{itemize}
  \item During model training, batch normalization continuously adjusts the intermediate output of the neural network by utilizing the mean and standard deviation of the minibatch, so that the values of the intermediate output in each layer throughout the neural network are more stable.
  \item The batch normalization methods for fully-connected layers and convolutional layers are slightly different.
  \item Like a dropout layer, batch normalization layers have different computation results in training mode and prediction mode.
  \item Batch normalization has many beneficial side effects, primarily that of regularization. On the other hand, the original motivation of reducing internal covariate shift seems not to be a valid explanation
\end{itemize}

\vfill
\bibliography{../References}
\nocite{zhang2020dive}
\end{document}

